---
layout: archive
title: "Posts"
permalink: /posts/
author_profile: true
---

One day I may find time to finally make a blog, but for now I am using this space to collect and link to some of the (quite elaborate) Twitter threads I've made explaining some papers! (find me @AliciaCurth!)

**October 2024: Benign Overfitting**. _When Double Descent & Benign Overfitting became a thing, I was a masters student in statistics — and so confused. I couldn't reconcile what l had literally just learned about bias-variance&co with modern ML 😢 Here's what I wish someone had told me then:_ 🧵 Continued [here](https://x.com/AliciaCurth/status/1841817856142348529). 📖 Corresponding [paper.](https://arxiv.org/abs/2409.18842)

**Feb 2024: Random Forests**. _Why do Random Forests perform so well off-the-shelf & appear essentially immune to overfitting?!? I’ve found the text-book answer “it’s just variance reduction 🤷🏼‍♀️” to be a bit too unspecific, so we investigate 🕵️‍♀️..._ 🧵 Continued [here](https://x.com/AliciaCurth/status/1762126839114310136). 📖 Corresponding [paper.](https://arxiv.org/abs/2402.01502)

**Nov 2023: Double Descent**. _Every StatML intro class covers complexity-error U-curves, so we asked ourselves whether the info from theses classes would be enough to explain double descent too? Our NeurIPS23 paper does a roundtrip of the Elements of statistical learning and answers "Yes"!_ 🧵 Continued [here](https://x.com/aliciacurth/status/1721887963347632504?s=12).📖 Corresponding [paper.](https://arxiv.org/abs/2310.18988)
