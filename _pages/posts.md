---
layout: archive
title: "Posts"
permalink: /posts/
author_profile: true
---

One day I may find time to finally make a blog, but for now I am using this space to collect and link to some of the (quite elaborate) Twitter threads I've made explaining some papers! (find me @AliciaCurth!)

**October 2024: Benign Overfitting**. _When Double Descent & Benign Overfitting became a thing, I was a masters student in statistics â€” and so confused. I couldn't reconcile what l had literally just learned about bias-variance&co with modern ML ğŸ˜¢ Here's what I wish someone had told me then:_ ğŸ§µ Continued [here](https://x.com/AliciaCurth/status/1841817856142348529). ğŸ“– Corresponding [paper.](https://arxiv.org/abs/2409.18842)

**Feb 2024: Random Forests**. _Why do Random Forests perform so well off-the-shelf & appear essentially immune to overfitting?!? Iâ€™ve found the text-book answer â€œitâ€™s just variance reduction ğŸ¤·ğŸ¼â€â™€ï¸â€ to be a bit too unspecific, so we investigate ğŸ•µï¸â€â™€ï¸..._ ğŸ§µ Continued [here](https://x.com/AliciaCurth/status/1762126839114310136). ğŸ“– Corresponding [paper.](https://arxiv.org/abs/2402.01502)

**Nov 2023: Double Descent**. _Every StatML intro class covers complexity-error U-curves, so we asked ourselves whether the info from theses classes would be enough to explain double descent too? Our NeurIPS23 paper does a roundtrip of the Elements of statistical learning and answers "Yes"!_ ğŸ§µ Continued [here](https://x.com/aliciacurth/status/1721887963347632504?s=12).ğŸ“– Corresponding [paper.](https://arxiv.org/abs/2310.18988)
